model:
  base: "unsloth/DeepSeek-R1-Distill-Llama-8B"
  max_seq_length: 2048
  quantization: 4bit
  lora:
    rank: 16
    target_modules: ["q_proj", "v_proj"]

training:
  batch_size: 2
  gradient_accumulation: 4
  learning_rate: 2e-5
  checkpoint_interval: 50
